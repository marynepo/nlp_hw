{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашняя работа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/marynepo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import requests\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пройдемся по страницам с отзывами в кинопоиске, скачаем тексты отзывов, деля их на положительные и отрицательные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "revs = {'pos':[], 'neg': []}\n",
    "response = requests.get('https://www.kinopoisk.ru/film/401177/reviews/ord/date/status/bad/perpage/50/', headers={'User-agent':'Mozilla/5.0'}).text\n",
    "soup = BeautifulSoup(response,'html.parser')\n",
    "for l in soup.find_all('span', {'class':'_reachbanner_', 'itemprop':'reviewBody'})[:50]:\n",
    "    revs['neg'].append(l.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.kinopoisk.ru/film/401177/reviews/ord/date/status/good/perpage/50/', headers={'User-agent':'Mozilla/5.0'}).text\n",
    "soup = BeautifulSoup(response,'html.parser')\n",
    "for l in soup.find_all('span', {'class':'_reachbanner_', 'itemprop':'reviewBody'})[:50]:\n",
    "    revs['pos'].append(l.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отделим тестовую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = revs['pos'][40:] + revs['neg'][40:]\n",
    "Y_test = ['pos']*10 + ['neg']*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем и лемматизируем отзывы и составляем словарь со словами, которые встречаются либо только в положительных, либо только в отрицательных отзывах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_voc(text):\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        if str(morph.parse(',')[0].tag) is 'PNCT':\n",
    "            words.append(morph.parse(t)[0].normal_form)\n",
    "            \n",
    "    return words\n",
    "            \n",
    "def make_voc(texts):\n",
    "    \n",
    "    pos_words = Counter()\n",
    "    neg_words = Counter()\n",
    "    \n",
    "    for t in texts['pos'][:40]:\n",
    "        pos_words.update(words_to_voc(t))\n",
    "    for t in texts['neg'][:40]:\n",
    "        neg_words.update(words_to_voc(t))\n",
    "        \n",
    "    pos_w = set(pos_words.keys()) - set(neg_words.keys())\n",
    "    neg_w = set(neg_words.keys()) - set(pos_words.keys())\n",
    "    \n",
    "    words = {'pos':[], 'neg': []}\n",
    "    \n",
    "    for w in pos_w:\n",
    "        if pos_words[w] > 2:\n",
    "            words['pos'].append(w)\n",
    "            \n",
    "    for w in neg_w:\n",
    "        if neg_words[w] > 2:\n",
    "            words['neg'].append(w)\n",
    "    \n",
    "    return words\n",
    "\n",
    "voc = make_voc(revs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию, которая предсказывает, положительный ли отзыв или отрицательный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_text(text):\n",
    "    words = words_to_voc(text) #можно еще сделать сет из этого, чтобы не учитывать повторение, но у меня так accuracy хуже\n",
    "    n = 0\n",
    "    p = 0\n",
    "    for w in words:\n",
    "        if w in voc['pos']:\n",
    "            p += 1\n",
    "        elif w in voc['neg']:\n",
    "            n += 1\n",
    "    if p >= n:\n",
    "        return 'pos'\n",
    "    else:\n",
    "        return 'neg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И проверим accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "def check_acc(x, y):\n",
    "    res = []  \n",
    "    for text in x:\n",
    "        res.append(mark_text(text))\n",
    "    print(\"Accuracy: %.4f\" % accuracy_score(res, y))\n",
    "    \n",
    "check_acc(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Улучшение:\n",
    "\n",
    "1) Учитывать частотность слов и добавить веса в соответствии с ними\n",
    "\n",
    "2) Посмотреть на n-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
